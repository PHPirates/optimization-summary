%%
%% Author: s156757
%% 26-10-2017
%%

% Preamble
\documentclass{exam}

\title{Summary of Linear Optimization 2WO20 \\ \small According to the summary of topics that you need to know for the exam}
\author{Thomas Schouten}

\usepackage{../src/optimization}

% Document
\begin{document}

    \maketitle

    \section{Introduction}\label{sec:introduction}

    \begin{define}
        A \indx{linear optimization problem} consists of optimizing a linear objective function given linear inequality constraints.
    \end{define}
    
    \begin{question}
        Reduce the linear optimization problem
        \[
            \min/\max \{ zx \mid Px \le u,\ Qx \ge v,\ Rx =w,\ x \in \reals^n \}
        \]
        for example
        \[
            \max \{ 5x + z \mid -x +y \ge 2,\ x + 4y + z \le 3,\ x,y,z \ge 0,\ x \in \reals^3 \}
        \]
        to standard form.
    \end{question}
    \begin{answer}
        The standard form is
        \[
            \max \{ cx \mid Ax\le b,\ x \in \reals^n \}
        \]
        and because the problem given is a list of equations, the answer is
        \[
            \max \{ 5x + z \mid x-y \le -2,\ x +4y+z \le 3,\ -x \le 0,\ -y\le 0,\ -z\le 0,\ x \in \reals^3 \}.
        \]
    \end{answer}
    \begin{theorem}[Fredholms Alternative]
        If $A$ is an $m\times n$ real matrix and $b \in \reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item There exists a column vector $x \in \reals^n$ such that $Ax=b$.
            \item There exists a row vector $y \in \reals^m$ such that $yA=0$ and $yb=1$.
        \end{enumerate}
    \end{theorem}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 5&0 \\ 1 & 2 & 1 \\ 1 & 1 & 2},\ b=\matrix{2 \\ 2 \\ 3}.
        \]
    \end{question}
    \begin{answer}
        We do Gaussian elimination on $[I|A|b]$, which you can read as $IAx=b$.
        \[
            \matrix[ccc|ccc|c]{
            1 & 0 & 0 & 1 & 5 & 0 & 2 \\
            0 & 1 & 0 & 1 & 2 & 1 & 2 \\
            0 & 0 & 1 & 1 & 1 & 2 & 3 \\
            }
            \sim
            \matrix[ccc|ccc|c]{
            - \frac 3 2 & 5 & - \frac 5 2 & 1 & 0 & 0 & - \frac 1 2 \\
            \frac 1 2 & -1 & \frac 1 2 & 0 & 1 & 0 & \frac 1 2 \\
            \frac 1 2 & -2 & \frac 1 2 & 0 & 0 & 1 & \frac 3 2 \\
            }
        \]
        so $x = \matrix{- \frac 1 2 \\ \frac 1 2 \\ \frac 3 2}$.
    \end{answer}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 2 \\ 4 & -3 \\ 5 & -1},\ b=\matrix{3 & 1 & 3}.
        \]
    \end{question}
    \begin{answer} Similarly to the last question,
        \[
            \matrix[ccc|cc|c]{
            1 & 0 & 0 & 1 & 2 & 3 \\
            0 & 1 & 0 & 4 & -3 & 1 \\
            0 & 0 & 1 & 5 & -1 & 3 \\
            }
            \sim
            \matrix[ccc|cc|c]{
            \frac 3 {11} & \frac 2 {11} & 0 & 1 & 0 & 1\\
            \frac 4 {11} & - \frac 1 {11} & 0 & 0 & 1 & 1 \\
            1 & 1 & -1 & 0 & 0 & 1 \\
            }
        \]
        So now we see that $IAx=b \sim \matrix{c_1 & 1 & 0 \\ c_2 & 0 & 1 \\ c_3 & 0 & 0} = \matrix{1 \\ 1 \\ 1}$ which includes the equation $\matrix{c_3 \cdot 0 & c_3 \cdot 0}\cdot \matrix{x \\ y} = 1$ which is impossible.
        We now look at the row which made this impossible, and take $y=\matrix{1 & 1 & -1}$, and verify that $yA=0$ and $yb=1$.
    \end{answer}

    \section{Chapter 1}\label{sec:chapter1}
    
    \begin{define}
        A set is \indx{convex} if $[x,y] \subset C$ for all $x,y \in C$, where $[x,y]$ is the line segment $\{ \lambda x + (1-\lambda)y : \lambda \in [0,1] \}$.
    \end{define}
    \begin{define}
        A set $C$ is a \indx{cone} if $\alpha x + \beta y \in C$ for all $x,y \in C$ and $\alpha, \beta >0$.
    \end{define}
    \begin{define}
        A set $H$ is a \indx{hyperplane} if there exist a $d \in \reals^n$ and $\delta \in \reals$ such that
        \[
            H = H_{d, \delta} = \{ x \in \reals^n : d x = \delta \}.
        \]
    \end{define}
    \begin{define}
        Let $H_{d,\delta}^{\le} = \{ x \in \reals^n : dx \le \delta \}$ be a \indx{halfspace}, defined similarly for $\ge, <, >$.
        $H_{d, \delta}$ is a \indx{separating hyperplane} of sets $X$ and $Y$ if $X \subset H_{d,\delta}^\le$ and $Y \subset H_{d,\delta}^\ge$.
        It is a \indx{strongly separating hyperplane} if $X \subset H_{d,\delta}^<$ and $Y \subset H_{d,\delta}^>$.
    \end{define}
    \begin{theorem}[Separation Theorem]
        Let $C \subset \reals^n$ be a closed, convex set, $x \in \reals^n$.
        Then
        \[
            x \not \in C \implies \text{there exists a hyperplane $H$ such that $H$ separates $\{x\}$ from $C$}.
        \]
    \end{theorem}
    \begin{proof}
        Take a closed radius around $x$, then the intersection $I$ of the ball and $C$ is closed and bounded so compact by Heine-Borel.
        Then the minimum of $||i-x||$ for $i \in I$ is attained by Weierstrass.
        Let $Z$ be the point in $I$ for which this distance is minimal.
        Take $d=(z-x)^\t $ and $\delta = \frac 1 2 d (z+x)$, then we find out if $dx>\delta$.
        Note that in general $||x||=xx^\t$.
        \[
            dx = d(\frac 1 2 (x+z) + \frac 1 2 (x-z)) = \frac 1 2 d (x+z) - \frac 1 2 d(z-x) = \delta - \frac 1 2 (z-x)^\t (z-x) = \delta - \delta - \frac 1 2 ||d||^2 < \delta.
        \]
        So now we have to prove that $dy>\delta$ for all $y \in C$.
        Suppose not for an $y$, then we need to show that $||y-x||<||z-x||$.
        Define $f(\lambda) = ||z-\lambda (y-z) -x ||^2$, which is a bit like the distance between a point on $[z,y]$ and $x$ because $z-\lambda(y-z) \in [z,y]$.
        Now we can write
        \begin{align*}
            f(\lambda)&= ((z-x)-\lambda(y-z))((z-x)-\lambda (y-z))^\t \\
            &=((z-x)-\lambda(y-z))\left( (z-x)^\t - \lambda (y-z)^\t  \right) \quad \text{because in general $(x-y)^\t = x^\t - y^\t$} \\
            &= (z-x)(z-x)^\t - \lambda (z-x)(y-z)^\t - \lambda (y-z)(z-x)^\t + \lambda^2 (y-z)(y-z)^\t \\
            &= ||z-x||^2 - 2\lambda (y-z) (z-x)^\t + \lambda ^2 ||y-z||^2 \quad \text{because in general $xy^\t = yx^\t $} \\
            &= ||d||^2 -2\lambda (y-z) d + \lambda ^2 ||y-z||^2.
        \end{align*}
        So
        \[
            f'(\lambda)= -2 (y-z) + 2 \lambda ||y-z||^2
        \]
        and
        \[
            f'(0) = -2 (y-z)d = -2 ||y-z||\cdot ||z-x|| \cos \alpha
        \]
        where $\alpha$ is the angle between $y-z$ and $d$.
        Now we see that since $y$ is on the other side of the hyperplane $dx=\delta$, the angle must be smaller than 90 degrees.
        Therefore $\cos \alpha >0$ and thus $f'(0)<0$.
        So for small $\lambda >0$ we have that $f(\lambda) < f(0)$.
        So
        \[
             ||z-x|| = f(0) > f(\lambda) = ||z-\lambda (y-z) -x ||
        \]
        and $u \coloneqq z-\lambda (y-z) \in [z,y]$ so $u \in C$.
        But $||u-x|| < ||z-x||$ so $z$ was not the point in $C$ closest to $x$.~$\lightning$
    \end{proof}
    \begin{theorem}[Farkas Lemma]
        Let $a_1,\dots ,a_n$ and $b$ be column vectors in $\reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item $b \in \cone\{a_1,\dots,a_n\}$
            \item There is a row vector $d \in \reals^m$ such that $d a_i \ge 0$ for all $i$ and $db <0$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Suppose they are both true, so there exist $\lambda_i$ such that $\sumin \lambda_i a_i = b$ for $\lambda_i \ge 0$ and there is a $d$ such that $d a_i \ge 0$ and $db <0$.
        Then
        \[
            0 > db = d (\sumin \lambda_i a_i) = \sumin \lambda_i (d a_i) \ge 0\ \lightning
        \]
        Now we prove that at least one is true.
        Suppose 1 is not true, so $b \not \in \cone\{a_1,\dots,a_n\}$.
        By the separation theorem there exists a $d' \in \reals^m$ such that $d' a_i \ge 0$ for $a \in \cone\{a_1,\dots,a_n\}$ and $d'b <0$.
        Since $a_1,\dots,a_n \in \cone\{a_1,\dots,a_n\}$ we have $d' a_i \ge 0$ for all $i$ so 2 holds.
    \end{proof}
    \begin{question}
        Derive one variant of Farkas Lemma from an other.
    \end{question}
    \begin{answer}
        Good luck.
    \end{answer}
    \begin{theorem}[Farkas Lemma, variant]
        Let $A$ be an $m \times n$ matrix, $b \in \reals^m$, then exactly one of the following holds.
        \begin{enumerate}
            \item There exists an $x \in \reals^n$ such that $Ax\le b$
            \item There exists an $y \in \reals^m$ such that $yA=0,\ y\ge 0,\ yb<0$.
        \end{enumerate}
    \end{theorem}
    \begin{theorem}[Carath\'eodory's Theorem]
        Let $S \subset \reals^n$ be a finite set of vectors, if $x \in \cone S$ then there exists a linearly independent set $T \subset S$ such that $x \in \cone T$.
    \end{theorem}
    \begin{proof}
        Suppose $T$ was not linearly independent, then the $t \in T$ are dependent so you can write one $t$ as a linear combination of the others and subtract it.
        In other words, there exist $\mu_t \in \reals$ not all zero such that $\sum_{t \in T} \mu_t t =0$.
        Now we can combine the linear combinations, such that
        \begin{align*}
             \sum \lambda_t t &= x  \\ + \alpha \sum \mu_t t &= 0 \\ \hline \hline
            \sum (\lambda_t + \alpha \mu_t) t &= x
        \end{align*}
        Note that all $\lambda_i >0$ because if $\lambda_i = 0$ then we could remove the corresponding $t$ from $T$ and we would have a smaller $T$.

        So for $\alpha$ small enough, $\lambda_t + \alpha \mu_t >0$.
        Now let $\alpha$ grow until for one $t$ we have $\lambda_t + \alpha \mu_t =0$.
        But then there is a linear combination of $t$'s where one coefficient ($\lambda_t + \alpha \mu_t$) is zero, so we did not need that $t \ \lightning$.
    \end{proof}

    \section{Chapter 2}\label{sec:chapter2}
    \begin{theorem}[Strong Duality]
        \[
            \max \{ cx \mid Ax \le b \} = \min \{ yb \mid yA=c,\ y \ge 0 \}
        \]
        if the maximization problem is feasible and bounded.
    \end{theorem}
    \begin{proof}
        % todo
    \end{proof}
    \begin{question}
        Find the dual of $\min \{ yb \mid yA \ge c,\ y \ge 0 \}$.
    \end{question}
    \begin{answer}
        We try to write it in the form $\min \{ y'b \mid y'A'=c,\ y'\ge0 \}$, for example by introducting a slack variable $S$ so that the problem becomes
        \begin{align*}
            &\min \{ yb \mid yA -SI=c,\ y \ge 0,\ S \ge 0 \} \\
            =&\min \{ \matrix{y & S} \matrix{b \\ 0} \mid \matrix{y & S} \matrix{A \\ -I} = c,\ \matrix{y & S} \ge 0 \} \\
            =&\max \{ cx \mid \matrix{A \\ -I} x \le \matrix{b \\ 0} \} \\
            =&\max \{ cx \mid Ax\le b,\ x \ge 0 \}.
        \end{align*}
    \end{answer}
    \begin{theorem}[Complementary Slackness]
        Referring to the situation of strong duality,
        \[
            cx = yb \iff \text{ both solutions are optimal } \iff (Ax)_i = b_i \text{ or } y_i = 0\ \forall_i
        \]
        This means that if $x^*$ is an optimal solution to the primal problem, $y^*$ for the dual, if we plug $x^*$ into the $i$'th inequality of the primal problem and it turns out to have slack (``speling'' in Dutch), which means that equality does not hold, then the theorem says that $y_i^*$ has to be 0.

        This also holds in reverse, so if the $j$'th inequality of the dual has slack for $y^*$, then $x_j^* = 0$.
        Hence the name `complementary slack relation`.
    \end{theorem}
    \begin{question}
        Derive the optimal dual solution from the optimal primal solution $(5, 15)$ of the problem
        \[
            \min \{ 5x + 10y \mid x_1 + 3y\le 50,\ 4x+2y \le 60,\ x_1 \le 5,\ x_1,x_2 \ge 0 \}.
        \]
    \end{question}
    \begin{answer}
        If we plug $(5,15)$ into the primal problem we see that the first inequality is tight but for the second $4\cdot 5 + 2 \cdot 15 = 50 < 60$ so there is slack here, so $y_2^* =0$.
        
        The dual of this problem is 
        \[
            \max \{ 50 y_1 + 60 y_2 + 5 y_3 \mid y_1 + 4 y_2 + y_3 \ge 5,\ 3y_1 + 2y_2 \ge 10,\ y \ge 0 \}
        \]
        so since $x_2^* \neq 0$ we know that the corresponding inequality of the dual must be tight, so $3y_1^* + 2 \cdot 0 = 10$ so $y_1^* = 10/3$.
        Also $x_1^* \neq 0$ so $10/3 + y_3^* =5$ so $y^* = (10/3, 0, 5/3)$.
    \end{answer}
    \begin{theorem}
        Any linear optimization problem has a basic optimal solution.
    \end{theorem}

    \section{Chapter 3}\label{sec:chapter3}

    \begin{define}

        $\sumin \lambda_i x_i$ is an \indx{affine combination} of $x_i$ if $\sumin \lambda_i = 1$.

        $\sumin \lambda_i x_i$ is a \indx{convex combination} of $x_i$ if $\sumin \lambda_i = 1$ and $\lambda_i \ge 0$.

        The \indx{convex hull} of a set of vectors is the set of all convex combinations of subsets.
        In two dimensions, think of a flat surface spanned by points.

        A \indx{polyhedron} is $\{ x : Ax \le b \}$, the solution of a finite set of inequalities.

        A \indx{polytope} is the convex hull of a finite set of points.

        $F$ is a \indx{face} of a polyhedron $P$ if $F=P \cap H$ for some supporting hyperplane $H$, which touches $P$ but does not separate.

        A face $F$ is a \indx{facet} of $P$ if $\dim F = \dim P -1$.

        An \indx{edge} of $P$ is a bounded face $F$ such that $\dim F =1$.

        A \indx{ray} is an unbounded edge.

        A point $v \in P$ is a \indx{vertex} if $\{v\}$ is a face of $P$. $\V(P)$ is the set of vertices of $P$.

        The \indx{lineality space} of $P$ is the linear space $\lin(P) \coloneqq \{ d \in \reals^n : \{ x + \lambda d : \lambda \in \reals \} \subset P \ \forall_{x \in P} \}$, so the set of directions $d$ for which for all points $x \in P$ we have that $x + \lambda d$ is also in $P$.

        The \indx{cone of directions} of $P$ is the lineality space for $\lambda \ge 0$.

    \end{define}
    \begin{theorem}
        If $P$ is a bounded polyhedron, $P = \convhull \V(P)$.
    \end{theorem}
    \begin{proof}
        Assumpe $P$ is non-empty.
        \begin{itemize}
            \item ``$\supseteq$'' $\V(P) \subset P$ because $\forall_{v \in \V(P)} : \{ v\} = P \cap H$ for a hyperplane $H$.
            Because $P$ is convex ($P$ is the intersection of convex halfspaces), $\convhull \V(P) \subset P$.
            \item ``$\subseteq$'' Suppose not, let $x \in P \backslash \convhull \V(P) $.
            Then by the separation theorem there exist $d \in \reals^n,\ \delta \in \reals$ such that $H_{d,\delta}$ strongly separates $\convhull \V(P)$ and $x$.
            Without loss of generality say $d y < \delta $ for all $y \in \convhull \V (P)$ and $dx > \delta$.
            Since $P$ is bounded, $\max \{ dy : y \in P \}$ is attained.
            So
            \[
                \max \{ d y : y \in \V (P) \} \le \max \{ dy : y \in \convhull \V (P) \} < \delta < dx \le \max \{ dz : z \in P \}
            \]
            but by a lemma we know that $\max \{ dz : z \in P \}$ is attained by a vertex of $P$. $\lightning$.
        \end{itemize}
    \end{proof}
    \begin{theorem}[Minkowski-Weyl]
        Let $C$ be a cone, then $C$ is polyhedral if and only if $C$ is finitely generated, which means that it is defined by a finite number of vectors.
    \end{theorem}

    \section{Chapter 4}\label{sec:chapter4}
    \begin{notation}
        We write $\max \{ cx+d:Ax=b,\ x\ge 0 \}$ as a \indx{tableau}
        \[
            T =
            \begin{tabular}{c|c}
                -c & d \\ \hline
                A & b
            \end{tabular}
        \]
    \end{notation}
    \begin{define}
        $T$ is a \indx{basic tableau} for basis $B$ if the columns of $A$ indexed by $B$ form the identity matrix \textbf{and} $c_{j}=0$ for $j \in B$.
        $T$ is then a \indx{feasible tableau} if $b \ge 0$.
        $T$ is an \indx{optimal tableau} if $-c \ge 0$.
    \end{define}
    \begin{question}
        Execute the two-phase simplex method for
        \[
            \max \{ \matrix{ 5 & 4}x : \matrix{1 & 2 \\ 2 & 1} x = \matrix{3\\3},\ x \ge 0 \}.
        \]
    \end{question}
    \begin{answer}
        Let $x^{\{x,y\}}$ denote a basic solution for basis $B=\{x,y\}$.
        You can get these by looking in the row to the left of an element from the vector $b$, and when you see a 1 which is in a column of $A$ indexed by $B$, you write the element of $B$ in that location in the basic solution.

        We pivot on the number in the column where $-c$ is smallest, and where the ratio of the element of $b$ divided by the element of $A$ in a certain row is smallest.
        \begin{equation*}
        \begin{aligned}
            T =
            \begin{tabular}{c|c}
                -c & d \\ \hline
                A & b
            \end{tabular}
             =&
            \begin{tabular}{cccc|c}
                -5 & -4 & 0 & 0 & 0 \\ \hline
                1 & 2 & 1 & 0 & 3 \\
                \circled{2} & 1 & 0 & 1 & 3 \\
            \end{tabular}
            &\sim
            \begin{tabular}{cccc|c}
                0 & 0 & 1/2 & 2 & 9 \\ \hline
                0 & 1 & 1/3 & -1/3 & 1 \\
                1 & 0 & -1/6 & 2/3 & 1 \\
            \end{tabular}
            \\
            x^{\{3,4\}} =& \matrix{0 & 0 & 3 & 3}
             &x^{\{1,2\}} = \matrix{1 & 1 & 0 & 0} \\
        \end{aligned}
        \end{equation*}
    \end{answer}

    \subsection{Problems you can encounter for the simplex method}
    \subsubsection{The problem is not feasible at the start}
    \begin{question}
        Execute the two-phase simplex method for a problem with
        \[
            A = \matrix{1 & -2 & 1 \\ 4 & -5 & -6 \\ 7 & 8 & -9},\ b=\matrix{-1 \\ -3 \\ -10}.
        \]
    \end{question}
    \begin{answer}
        We see that $b < 0$ so the problem is infeasible when written this way.
        But here we can do something about it, we consider
        \[
            \begin{tabular}{ccc|c}
                0 & 0 & 1 & 0 \\
                A & I & -1 & b \\
            \end{tabular}
        \]
        and we pivot on the \textbf{biggest} ration of $b$ divided by an element of a column.
        To get back, we delete the extra 7'th column and replace the top row with $\matrix{-c & 0 & 0 & 0 &|& 0}$.
    \end{answer}
    \subsubsection{The problem appears infeasible when pivoting}
    For example
    \[
        \begin{tabular}{cccc|c}
            1/3 & 0 & 0 & 4/3 & 13 \\ \hline
            7/3 & 1 & 0 & 1/3 & 10/3 \\
            -1/3 & 0 & 1 & -1/3 & -1/3 \\
        \end{tabular}
    \]
    \subsubsection{The problem is unbounded}
    \begin{question}
        Execute the two-phase simplex method for
        \[
            \max \{ \matrix{7 & 3 & 2} x : \matrix{1 & 0 & -1 \\ -1 & 1 & 0 \\ 0 & -1 & 1} x \le \matrix{3 \\ 9  \\ 1},\ x \ge 0 \}
        \]
    \end{question}
    \begin{answer}
        \[
            T \sim \dots \sim
            \begin{tabular}{cccccc|c}
                0 & 0 & -12 & 10 & 3 & 0 & 57 \\ \hline
                1 & 0 & -1 & 1 & 0 & 0 & 3 \\
                0 & 1 & -1 & 1 & 1 & 0 & 12 \\
                0 & 0 & 0 & 1 & 1 & 1 & 13 \\
            \end{tabular}
        \]
        looks unbounded because you can add the last row to the first row indefinitely.
        We prove that the problem is unbounded by finding an $f$ such that $f \ge 0,\ Af=0$ and $cf >0$, in this case for example $f^\t = \matrix{1 & 1 & 1 & 0 & 0 & 0}$.
        Now we see that $x^{\{1,2,6\}} + \lambda f$ is a feasible solution, because
        \[
            A (x^{\{1,2,6\}} + \lambda f) = A x^{\{1,2,6\}} + \lambda A f = b + \lambda \cdot 0 = b
        \]
        and $x^{\{1,2,6\}} + \lambda f \ge 0$ for all $\lambda \ge 0$.
        But then $cx + \lambda cf$ is unbounded because $cf \ge 0$.
    \end{answer}

    \subsubsection{There are equal choices for columns or rows}
    \begin{theorem}[Bland's rule]
        If there are two equal choices for columns, choose the left-most column.
        If there are two equal choices for rows, choose the row such that the column that goes out of the base (that gets `messed up') is left-most.
        If you apply this rule, you will not encounter cycles.
    \end{theorem}
    \begin{proof}
        Is difficult. % todo
    \end{proof}

    \section{Chapter 5}
    \begin{theorem}[Farkas Lemma for integrality]
        Let $A$ be a rational $m \times n$ matrix, $b \in \rationals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item There is an $x \in \integers^n$ such that $Ax=b$.
            \item There is an $y \in \rationals^m$ such that $yA \in \integers^n$ and $yb \not \in \integers$.
        \end{enumerate}
    \end{theorem}
    Think of this as Farkas lemma with integrality instead of $\ge 0$.
    \begin{proof}
        Is long. % todo
    \end{proof}

    \begin{question}
        Give the set of integer solutions of
        \[
            \{ x \in \integers^4 : \matrix{6 & 9 & - 15 & 12 \\ 7 & 4 & 2 & 0} x = \matrix{3 \\ 1}.
        \]
    \end{question}
    \begin{answer}
        First recall the allowed operations for swiping an integer matrix.
        \begin{enumerate}
            \item Multiply a column with $-1$.
            \item Interchange columns.
            \item Add an \textbf{integer} multiple of a column to another column.
        \end{enumerate}
        If
        \[
            \matrix{A \\ I} \sim \matrix{H \\ U} = \matrix{B & 0 \\ V & W}
        \]
        where $H$ is in Hermite normal form, so $0 < b_{ij} < b_{ii}$ for $b_{ij}$ to the left of the diagonal.
        Also, $B$ needs to be a square matrix.
        Recall that
        \[
            B = \matrix {a & b \\ c & d} \implies B^{-1} = \frac{1}{ad-bc} \matrix{d & -b \\ -c & a}
        \]
        Now if $B^{-1}b$ is not integral then $Ax=b$ has no integral solutions.
        Otherwise,
        \[
            \{ x \in \integers^n : Ax=b \} = \{ VB^{-1}b + W y : y \in \integers^{n-m} \}
        \]
        So in this case
        \[
            \matrix{6 & 9 & -15 & 12 \\ 7 & 4 & 2 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1} \sim \dots \sim  \matrix{3 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 2 & 1 & -2 & 8 \\ -2 & -1 & 3 & 64 \\ -3 & -1 & 1 & 12 \\ -3 & -1 & 0 & -1}
        \]
        and $B^{-1}b = \frac 1 3 \matrix{1 & 0 \\ 0 & 3} = \matrix{1\\1}$ which is integral.
        So the solution set is
        \[
            \{ VB^{-1}b=\matrix{3\\-3\\-4\\-4} + \matrix{-2 & 8 \\ 3 & 64 \\ 1 & 12  \\ 0 & -1} y : y \in \integers ^2 \}
        \]
    \end{answer}
    \begin{define}
        A \indx{lattice} (`rooster' in Dutch) is a discrete additive subgroup of $\reals^m$, so a grid of points, characterized by the next theorem.
    \end{define}
    \begin{theorem}[Rational vectors generate a lattice.]
        Let $a_1,\dots,a_{n}\in \rationals ^m$.
        Then \[ L=\mathrm{int} \{  a_1,\dots,a_n \} = \{ \lambda_1 a_1 + \dots + \lambda_n a_n : \lambda_1,\dots,\lambda_n \in \integers \} \]
        is a lattice.
    \end{theorem}
    \begin{proof}
        Let $A$ be the matrix with $a_i$ as $i$'th column.
        Then $L = \{ Ax : x \in \integers ^n \}$ (think of $x$ as the vector containing the $\lambda$'s) and $A$ has linearly independent columns.
        In general, if $A \sim A'$ then $\{ Ax : x \in \integers ^n \} = \{ A'x': x' \in \integers ^n \}$, and we know there exists a $H \sim A$ in Hermite normal form, so $H = \matrix{B & 0}$ where $B$ is invertible.
        So
        \[
            L = \{ \matrix{B & 0} \matrix{u \\ v},\ \matrix{u\\v} \in \integers ^n \} = \{ Bu : u \in \integers^m \}
        \]
        so $L$ is also generated by the linearly independent columns of $B$.
        These vectors are integral, so they span a discrete additive subgroup of $\reals^n$, so $L$ is a lattice.
    \end{proof}

    \section{Chapter 6}

    \begin{define}
        The \indx{integer hull} of a polyhedron $P$ is $P_I = \convhull(P \cap \integers^n)$, so the set of all convex combinations of subsets of $P \cap \integers ^n$, which looks in 2D like the set you get when tightening the boundary of $P$ around the integral points closest to the boundary.

        $P$ is an \indx{integral polyhedron} when $P=P_I$, so when the vertices of $P$ are integral.

        A matrix $M$ is \indx{totally unimodular} when the determinant of all square submatrices of size $\ge 1$ is in the set $\{-1, 0, 1\}$.

        $\mathrm{Ex}(P)$ is the set of extreme points of $P$, so all $x$ for which $x \not \in \convhull(P\backslash\{x\})$.
    \end{define}
    \begin{theorem}
        Let $M$ be an $m \times n$ TU matrix, $b \in \integers ^m$, then $P=\{ x \in \reals^n : Mx \le b \}$ is an integral polyhedron.
    \end{theorem}
    \begin{proof}
        We need to prove that $P = \convhull (P \cap \integers ^n)$.
        \paragraph{$\supset$} $P \cap \integers ^n \subset P$ and $P$ is convex so $\convhull(P \cap \integers ^n) \subset P$.
        \paragraph{$\subset$} Let $y \in P$, $P'\coloneqq \{ x \in P : \floor{y} \le x \le \ceil{y}$.
        It is enough to show that $P'$ is integral, because then $y \in \convhull (P'\cap \integers ^n)$ and because $P' \subset P \implies P' \cap \integers ^n \subset P \cap \integers ^n $ then $y \in \convhull( P \cap \integers ^n)$.
        $P'$ is a bounded polyhedron so $P'=\{ x \in \reals^n : M'x \le b' \}$ where
        \[
            M'= \matrix{M \\ I \\ -I},\ b'=\matrix{b \\ -\floor{y} \\ \ceil{y}}.
        \]
        So $M'$ is also TU so $\mathrm{Ex}(P') \subset \integers ^n$.
        So $P'= conv.hull \mathrm{Ex}(P') \implies P' = \convhull(P'\cap \integers ^n)$.
    \end{proof}
    \begin{define}
        A graph $G(V,E)$ is \indx{bipartite} if it has no cycles of odd length.

        A \indx{matching} is a set of edges without common vertices, $\nu (G) = \max \{ |F| : F \subset E \text{ is a matching in } G \}$.

        A \indx{vertex cover} is a set of vertices such that each edge is incident with (connected to) at least one vertex of this set, $\tau (G) = \min \{ |U|: U \subset V \text{ is a vertex cover of } G \}$.

        $M^G$ is the \indx{incidence matrix} of $G$, which contains for every vertex $v \in V$ (indexed in the rows) and edge $e \in E$ (indexed in the columns)
        \[
            M_{v,e}^G =
            \begin{cases}
                1 & \text{ if $v$ is an endvertex of $e$} \\
                0 & \text { otherwise } \\
            \end{cases}
        \]
    \end{define}
    \begin{theorem}[K\"onigs theorem]
        If $G$ is bipartite, then $\nu (G) = \tau (G)$.
    \end{theorem}
    \begin{proof}
        Let $F$ be a matching, it is a set of edges so can be represented by a vector $x$ with length $|E|$, where $x = \begin{cases} 1 & \text{ if } e \in F \\ 0 & \text{ if not } \end{cases}$.

        Then each vertex has to be incident to at most one edge of $F$, so $M^G x \le 1$, so
        \[
            \nu (G) = \max \{ \one^\t x : M^G x \le \one^\t,\ x \ge 0 \}.
        \]
        Let $U$ be a vertex cover, so each edge is incident with at most one $v \in U$, then since it is a subset of $V$ so can be represented by a vector $y$ in the same way as above, so
        \[
            \tau (G) = \min \{ y \one : y M^G \le \one^\t,\ y \ge 0 \}.
        \]
        By linear optimalization duality and because the matrices are TU it follows that $\nu (G) = \tau (G)$.
    \end{proof}

    \section{Chapter 7}

    \begin{define}
        A function $f$ is \indx{convex} if $f(x + \lambda (y-x)) \le f(x) + \lambda ( f(y)-f(x))$, $\lambda \in (0,1)$.
    \end{define}
    \begin{theorem}
        A differentiable function $f$ is convex if and only if $f(y) \ge f(x) + \nabla f(y-x)$.
    \end{theorem}
    \begin{define}
        The \indx{Hessian} of $f$ at $x$ is
        \[
            \nabla^2 f(x) = \matrix{
            \frac{\partial^2 f}{\partial x_1^2} (x) & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} (x) \\
            \vdots &\ddots &\vdots \\
            \frac{\partial^2 f}{\partial x_n \partial x_1} (x)  & \dots & \frac{\partial^2 f}{\partial x_n^2} (x) }
        \]
    \end{define}
    \begin{question}
        Prove that a certain function $f$ is convex, for example $f(x) = ||x-c||+3||x-d||$.
    \end{question}
    \begin{answer}
        Recall that when $f$ is convex, the following are also convex.
        \begin{itemize}
            \item $x \mapsto \alpha f(x)$, $\alpha >0$
            \item $x \mapsto f(x+t)$
            \item $x \mapsto f(Ax)$
        \end{itemize}
        so since $x \mapsto ||x||$ is convex, $||x-c||$ is convex etc.

        If you need to find a counterexample, try to find $x,y$ and $\lambda$ as in the definition of a convex function.
    \end{answer}
    \begin{answer}
        Another possibility is to calculate the Hessian and find out if it is PSD.
    \end{answer}
    \begin{define}
        A symmetric matrix $A$ is \indx{positive semidefinite} if and only if $x^\t A x \ge 0$ for all $x$.
    \end{define}
    But it is easier to think of a PSD matrix as characterised in the next theorem.
    \begin{theorem}
        Let $A$ be a symmetric matrix, then the following are equivalent.
        \begin{enumerate}
            \item $A$ is PSD
            \item All eigenvalues are nonnegative.
            \item $A=Z^\t Z$ for a real matrix $Z$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
%        \paragraph{(1) $\implies$ (2)} Let $A$ be PSD, let $\lambda$ be an eigenvalue of $A$ and $f$ the corresponding eigenvector, so $Af=\lambda f$.
%        If we multiply both sides with $f^\t$ then $f^\t A f = \lambda ||f||^2$.
%        But $A$ is PSD so by definition $f^\t A f \ge 0$, so since $||f||^2 \ge 0$ it must be that $\lambda \ge 0$.
%        \\
%        \paragraph{$(2) \implies (3)$} (2) $\implies$ (3)\ Square roots of nonnegative numbers are real, so $\sqrt{\lambda_i} \in \reals$.
%        Since $A$ is symmetric there exists a orthonormal basis of eigenvectors, put these as columns in a matrix $F$.
%        Now $F^\t F = I$ because $f^\t f = ||f||$, and $F^\t A F$ is equal to the matrix with on the diagonal the eigenvalues of $A$. $f^\t \cdot \lambda f = \lambda ||f|| = \lambda$.
%        Take
%        \[
%            Z = \matrix{\sqrt{\lambda_1} & \dots & 0 \\
%            \vdots & \ddots & \vdots \\
%            0 & \dots & \sqrt{\lambda_n}} F^{-1}
%        \]
%        Note that in general $\left( A^{-1} \right)^\t = \left( A^\t \right)^{-1}$.
%        Now $Z^\t Z = \left( F^\t \right)^{-1} F^{-1} L$
%        where $L$ is the matrix with on the diagonal the $\lambda_i$.
%        This should be equal to $A$. % todo
    \end{proof}
    \begin{question}
        Find out if the matrix
        \[
            A = \matrix{ 1 & 2 & -1 \\ 2 & 5 & 1 \\ -1 & 1 & 12}
        \]
        is PSD.
    \end{question}
    \begin{answer}
        Recall the allowed operations for `symmetric swiping':
        \begin{enumerate}
            \item Multiplying the $i$'th row and $i$'th column by $\lambda \neq 0$.
            Note that the element on the diagonal will be multipliedby $\lambda^2$.
            \item Swapping the $i$'th and $j$'th column and swapping the $i$'th and $j$'th row.
            \item Adding $\lambda$ times the $i$'th colun to the $j$'th column and adding $\lambda$ times the $i$'th row to the $j$'th row.
        \end{enumerate}
        Now we swipe
        \[
            A \sim \dots \sim \matrix{1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 11}
        \]
        and notice that the eigenvalues, which are on the diagonal, are positive so $A$ is PSD\@.
        Remember then if you encounter a negative number on the diagonal when swiping, then this is a submatrix with $\det <0$ so the matrix cannot be PSD\@.
    \end{answer}
    The following theorem tells us that when the values next to the diagonal are small compared to the elements on the diagonal, then the matrix eigenvalues will behave approximately like a diagonal matrix.
    \begin{theorem}[Ger\v{s}gorin's Theorem]
        Given a complex $n \times n$ matrix $A$, let $\lambda$ be an eigenvalue of $A$.
        Let $\rho_p = \sum_{j=p} |a_{pj}|$ be the sum of absolute elements of the $p$'th row without the element on the diagonal.
        Then $\{ \lambda \in \complex : |\lambda - a_{pp}| \le \rho_p \}$ is the Ger\v{s}gorin disk.
    \end{theorem}

\end{document}