%%
%% Author: s156757
%% 26-10-2017
%%

% Preamble
\documentclass{exam}

\title{Summary of Linear Optimization 2WO20 \\ \small According to the summary of topics that you need to know for the exam}
\author{Thomas Schouten}

\usepackage{../src/optimization}

% Document
\begin{document}

    \maketitle

    \section{Introduction}\label{sec:introduction}

    \begin{define}
        A \indx{linear optimization problem} consists of optimizing a linear objective function given linear inequality constraints.
    \end{define}
    
    \begin{question}
        Reduce the linear optimization problem
        \[
            \min/\max \{ zx \mid Px \le u,\ Qx \ge v,\ Rx =w,\ x \in \reals^n \}
        \]
        for example
        \[
            \max \{ 5x + z \mid -x +y \ge 2,\ x + 4y + z \le 3,\ x,y,z \ge 0,\ x \in \reals^3 \}
        \]
        to standard form.
    \end{question}
    \begin{answer}
        The standard form is
        \[
            \max \{ cx \mid Ax\le b,\ x \in \reals^n \}
        \]
        and because the problem given is a list of equations, the answer is
        \[
            \max \{ 5x + z \mid x-y \le -2,\ x +4y+z \le 3,\ -x \le 0,\ -y\le 0,\ -z\le 0,\ x \in \reals^3 \}.
        \]
    \end{answer}
    \begin{theorem}[Fredholms Alternative]
        If $A$ is an $m\times n$ real matrix and $b \in \reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item There exists a column vector $x \in \reals^n$ such that $Ax=b$.
            \item There exists a row vector $y \in \reals^m$ such that $yA=0$ and $yb=1$.
        \end{enumerate}
    \end{theorem}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 5&0 \\ 1 & 2 & 1 \\ 1 & 1 & 2},\ b=\matrix{2 \\ 2 \\ 3}.
        \]
    \end{question}
    \begin{answer}
        We do Gaussian elimination on $[I|A|b]$, which you can read as $IAx=b$.
        \[
            \matrix[ccc|ccc|c]{
            1 & 0 & 0 & 1 & 5 & 0 & 2 \\
            0 & 1 & 0 & 1 & 2 & 1 & 2 \\
            0 & 0 & 1 & 1 & 1 & 2 & 3 \\
            }
            \sim
            \matrix[ccc|ccc|c]{
            - \frac 3 2 & 5 & - \frac 5 2 & 1 & 0 & 0 & - \frac 1 2 \\
            \frac 1 2 & -1 & \frac 1 2 & 0 & 1 & 0 & \frac 1 2 \\
            \frac 1 2 & -2 & \frac 1 2 & 0 & 0 & 1 & \frac 3 2 \\
            }
        \]
        so $x = \matrix{- \frac 1 2 \\ \frac 1 2 \\ \frac 3 2}$.
    \end{answer}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 2 \\ 4 & -3 \\ 5 & -1},\ b=\matrix{3 & 1 & 3}.
        \]
    \end{question}
    \begin{answer} Similarly to the last question,
        \[
            \matrix[ccc|cc|c]{
            1 & 0 & 0 & 1 & 2 & 3 \\
            0 & 1 & 0 & 4 & -3 & 1 \\
            0 & 0 & 1 & 5 & -1 & 3 \\
            }
            \sim
            \matrix[ccc|cc|c]{
            \frac 3 {11} & \frac 2 {11} & 0 & 1 & 0 & 1\\
            \frac 4 {11} & - \frac 1 {11} & 0 & 0 & 1 & 1 \\
            1 & 1 & -1 & 0 & 0 & 1 \\
            }
        \]
        So now we see that $IAx=b \sim \matrix{c_1 & 1 & 0 \\ c_2 & 0 & 1 \\ c_3 & 0 & 0} = \matrix{1 \\ 1 \\ 1}$ which includes the equation $\matrix{c_3 \cdot 0 & c_3 \cdot 0}\cdot \matrix{x \\ y} = 1$ which is impossible.
        We now look at the row which made this impossible, and take $y=\matrix{1 & 1 & -1}$, and verify that $yA=0$ and $yb=1$.
    \end{answer}

    \section{Chapter 1}\label{sec:chapter1}
    
    \begin{define}
        A set is \indx{convex} if $[x,y] \subseteq C$ for all $x,y \in C$, where $[x,y]$ is the line segment $\{ \lambda x + (1-\lambda)y : \lambda \in [0,1] \}$.
    \end{define}
    \begin{define}
        A set $C$ is a \indx{cone} if $\alpha x + \beta y \in C$ for all $x,y \in C$ and $\alpha, \beta >0$.
    \end{define}
    \begin{define}
        A set $H$ is a \indx{hyperplane} if there exist a $d \in \reals^n$ and $\delta \in \reals$ such that
        \[
            H = H_{d, \delta} = \{ x \in \reals^n : d x = \delta \}.
        \]
    \end{define}
    \begin{define}
        Let $H_{d,\delta}^{\le} = \{ x \in \reals^n : dx \le \delta \}$ be a \indx{halfspace}, defined similarly for $\ge, <, >$.
        $H_{d, \delta}$ is a \indx{separating hyperplane} of sets $X$ and $Y$ if $X \subseteq H_{d,\delta}^\le$ and $Y \subseteq H_{d,\delta}^\ge$.
        It is a \indx{strongly separating hyperplane} if $X \subseteq H_{d,\delta}^<$ and $Y \subseteq H_{d,\delta}^>$.
    \end{define}
    \begin{theorem}[Separation Theorem]
        Let $C \subseteq \reals^n$ be a closed, convex set, $x \in \reals^n$.
        Then
        \[
            x \not \in C \implies \text{there exists a hyperplane $H$ such that $H$ separates $\{x\}$ from $C$}.
        \]
    \end{theorem}
    \begin{proof}
        Take a closed radius around $x$, then the intersection $I$ of the ball and $C$ is closed and bounded so compact by Heine-Borel.
        Then the minimum of $||i-x||$ for $i \in I$ is attained by Weierstrass.
        Let $Z$ be the point in $I$ for which this distance is minimal.
        Take $d=(z-x)^\t $ and $\delta = \frac 1 2 d (z+x)$, then we find out if $dx>\delta$.
        Note that in general $||x||=xx^\t$.
        \[
            dx = d(\frac 1 2 (x+z) + \frac 1 2 (x-z)) = \frac 1 2 d (x+z) - \frac 1 2 d(z-x) = \delta - \frac 1 2 (z-x)^\t (z-x) = \delta - \delta - \frac 1 2 ||d||^2 < \delta.
        \]
        So now we have to prove that $dy>\delta$ for all $y \in C$.
        Suppose not for an $y$, then we need to show that $||y-x||<||z-x||$.
        Define $f(\lambda) = ||z-\lambda (y-z) -x ||^2$, which is a bit like the distance between a point on $[z,y]$ and $x$ because $z-\lambda(y-z) \in [z,y]$.
        Now we can write
        \begin{align*}
            f(\lambda)&= ((z-x)-\lambda(y-z))((z-x)-\lambda (y-z))^\t \\
            &=((z-x)-\lambda(y-z))\left( (z-x)^\t - \lambda (y-z)^\t  \right) \quad \text{because in general $(x-y)^\t = x^\t - y^\t$} \\
            &= (z-x)(z-x)^\t - \lambda (z-x)(y-z)^\t - \lambda (y-z)(z-x)^\t + \lambda^2 (y-z)(y-z)^\t \\
            &= ||z-x||^2 - 2\lambda (y-z) (z-x)^\t + \lambda ^2 ||y-z||^2 \quad \text{because in general $xy^\t = yx^\t $} \\
            &= ||d||^2 -2\lambda (y-z) d + \lambda ^2 ||y-z||^2.
        \end{align*}
        So
        \[
            f'(\lambda)= -2 (y-z) + 2 \lambda ||y-z||^2
        \]
        and
        \[
            f'(0) = -2 (y-z)d = -2 ||y-z||\cdot ||z-x|| \cos \alpha
        \]
        where $\alpha$ is the angle between $y-z$ and $d$.
        Now we see that since $y$ is on the other side of the hyperplane $dx=\delta$, the angle must be smaller than 90 degrees.
        Therefore $\cos \alpha >0$ and thus $f'(0)<0$.
        So for small $\lambda >0$ we have that $f(\lambda) < f(0)$.
        So
        \[
             ||z-x|| = f(0) > f(\lambda) = ||z-\lambda (y-z) -x ||
        \]
        and $u \coloneqq z-\lambda (y-z) \in [z,y]$ so $u \in C$.
        But $||u-x|| < ||z-x||$ so $z$ was not the point in $C$ closest to $x$.~$\lightning$
    \end{proof}
    \begin{theorem}[Farkas Lemma]
        Let $a_1,\dots ,a_n$ and $b$ be column vectors in $\reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item $b \in \cone\{a_1,\dots,a_n\}$
            \item There is a row vector $d \in \reals^m$ such that $d a_i \ge 0$ for all $i$ and $db <0$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Suppose they are both true, so there exist $\lambda_i$ such that $\sumin \lambda_i a_i = b$ for $\lambda_i \ge 0$ and there is a $d$ such that $d a_i \ge 0$ and $db <0$.
        Then
        \[
            0 > db = d (\sumin \lambda_i a_i) = \sumin \lambda_i (d a_i) \ge 0\ \lightning
        \]
        Now we prove that at least one is true.
        Suppose 1 is not true, so $b \not \in \cone\{a_1,\dots,a_n\}$.
        By the separation theorem there exists a $d' \in \reals^m$ such that $d' a_i \ge 0$ for $a \in \cone\{a_1,\dots,a_n\}$ and $d'b <0$.
        Since $a_1,\dots,a_n \in \cone\{a_1,\dots,a_n\}$ we have $d' a_i \ge 0$ for all $i$ so 2 holds.
    \end{proof}
    \begin{question}
        Derive one variant of Farkas Lemma from an other.
    \end{question}
    \begin{answer}
        Good luck.
    \end{answer}
    \begin{theorem}[Farkas Lemma, variant]
        Let $A$ be an $m \times n$ matrix, $b \in \reals^m$, then exactly one of the following holds.
        \begin{enumerate}
            \item There exists an $x \in \reals^n$ such that $Ax\le b$
            \item There exists an $y \in \reals^m$ such that $yA=0,\ y\ge 0,\ yb<0$.
        \end{enumerate}
    \end{theorem}
    \begin{theorem}[Carath\'eodory's Theorem]
        Let $S \subset \reals^n$ be a finite set of vectors, if $x \in \cone S$ then there exists a linearly independent set $T \subseteq S$ such that $x \in \cone T$.
    \end{theorem}
    \begin{proof}
        Suppose $T$ was not linearly independent, then the $t \in T$ are dependent so you can write one $t$ as a linear combination of the others and subtract it.
        In other words, there exist $\mu_t \in \reals$ not all zero such that $\sum_{t \in T} \mu_t t =0$.
        Now we can combine the linear combinations, such that
        \begin{align*}
             \sum \lambda_t t &= x  \\ + \alpha \sum \mu_t t &= 0 \\ \hline \hline
            \sum (\lambda_t + \alpha \mu_t) t &= x
        \end{align*}
        Note that all $\lambda_i >0$ because if $\lambda_i = 0$ then we could remove the corresponding $t$ from $T$ and we would have a smaller $T$.

        So for $\alpha$ small enough, $\lambda_t + \alpha \mu_t >0$.
        Now let $\alpha$ grow until for one $t$ we have $\lambda_t + \alpha \mu_t =0$.
        But then there is a linear combination of $t$'s where one coefficient ($\lambda_t + \alpha \mu_t$) is zero, so we did not need that $t \ \lightning$.
    \end{proof}

\end{document}