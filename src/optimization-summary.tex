%%
%% Author: s156757
%% 26-10-2017
%%

% Preamble
\documentclass{exam}

\title{Summary of Linear Optimization 2WO20 \\ \small According to the summary of topics that you need to know for the exam}
\author{Thomas Schouten}

\usepackage{../src/optimization}

% Document
\begin{document}

    \maketitle

    \section{Introduction}\label{sec:introduction}

    \begin{define}
        A \indx{linear optimization problem} consists of optimizing a linear objective function given linear inequality constraints.
    \end{define}
    
    \begin{question}
        Reduce the linear optimization problem
        \[
            \min/\max \{ zx \mid Px \le u,\ Qx \ge v,\ Rx =w,\ x \in \reals^n \}
        \]
        for example
        \[
            \max \{ 5x + z \mid -x +y \ge 2,\ x + 4y + z \le 3,\ x,y,z \ge 0,\ x \in \reals^3 \}
        \]
        to standard form.
    \end{question}
    \begin{answer}
        The standard form is
        \[
            \max \{ cx \mid Ax\le b,\ x \in \reals^n \}
        \]
        and because the problem given is a list of equations, the answer is
        \[
            \max \{ 5x + z \mid x-y \le -2,\ x +4y+z \le 3,\ -x \le 0,\ -y\le 0,\ -z\le 0,\ x \in \reals^3 \}.
        \]
    \end{answer}
    \begin{theorem}[Fredholms Alternative]
        If $A$ is an $m\times n$ real matrix and $b \in \reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item There exists a column vector $x \in \reals^n$ such that $Ax=b$.
            \item There exists a row vector $y \in \reals^m$ such that $yA=0$ and $yb=1$.
        \end{enumerate}
    \end{theorem}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 5&0 \\ 1 & 2 & 1 \\ 1 & 1 & 2},\ b=\matrix{2 \\ 2 \\ 3}.
        \]
    \end{question}
    \begin{answer}
        We do Gaussian elimination on $[I|A|b]$, which you can read as $IAx=b$.
        \[
            \matrix[ccc|ccc|c]{
            1 & 0 & 0 & 1 & 5 & 0 & 2 \\
            0 & 1 & 0 & 1 & 2 & 1 & 2 \\
            0 & 0 & 1 & 1 & 1 & 2 & 3 \\
            }
            \sim
            \matrix[ccc|ccc|c]{
            - \frac 3 2 & 5 & - \frac 5 2 & 1 & 0 & 0 & - \frac 1 2 \\
            \frac 1 2 & -1 & \frac 1 2 & 0 & 1 & 0 & \frac 1 2 \\
            \frac 1 2 & -2 & \frac 1 2 & 0 & 0 & 1 & \frac 3 2 \\
            }
        \]
        so $x = \matrix{- \frac 1 2 \\ \frac 1 2 \\ \frac 3 2}$.
    \end{answer}
    \begin{question}
        Find any $x$ or $y$ as above given
        \[
            A = \matrix{1 & 2 \\ 4 & -3 \\ 5 & -1},\ b=\matrix{3 & 1 & 3}.
        \]
    \end{question}
    \begin{answer} Similarly to the last question,
        \[
            \matrix[ccc|cc|c]{
            1 & 0 & 0 & 1 & 2 & 3 \\
            0 & 1 & 0 & 4 & -3 & 1 \\
            0 & 0 & 1 & 5 & -1 & 3 \\
            }
            \sim
            \matrix[ccc|cc|c]{
            \frac 3 {11} & \frac 2 {11} & 0 & 1 & 0 & 1\\
            \frac 4 {11} & - \frac 1 {11} & 0 & 0 & 1 & 1 \\
            1 & 1 & -1 & 0 & 0 & 1 \\
            }
        \]
        So now we see that $IAx=b \sim \matrix{c_1 & 1 & 0 \\ c_2 & 0 & 1 \\ c_3 & 0 & 0} = \matrix{1 \\ 1 \\ 1}$ which includes the equation $\matrix{c_3 \cdot 0 & c_3 \cdot 0}\cdot \matrix{x \\ y} = 1$ which is impossible.
        We now look at the row which made this impossible, and take $y=\matrix{1 & 1 & -1}$, and verify that $yA=0$ and $yb=1$.
    \end{answer}

    \section{Chapter 1}\label{sec:chapter1}
    
    \begin{define}
        A set is \indx{convex} if $[x,y] \subset C$ for all $x,y \in C$, where $[x,y]$ is the line segment $\{ \lambda x + (1-\lambda)y : \lambda \in [0,1] \}$.
    \end{define}
    \begin{define}
        A set $C$ is a \indx{cone} if $\alpha x + \beta y \in C$ for all $x,y \in C$ and $\alpha, \beta >0$.
    \end{define}
    \begin{define}
        A set $H$ is a \indx{hyperplane} if there exist a $d \in \reals^n$ and $\delta \in \reals$ such that
        \[
            H = H_{d, \delta} = \{ x \in \reals^n : d x = \delta \}.
        \]
    \end{define}
    \begin{define}
        Let $H_{d,\delta}^{\le} = \{ x \in \reals^n : dx \le \delta \}$ be a \indx{halfspace}, defined similarly for $\ge, <, >$.
        $H_{d, \delta}$ is a \indx{separating hyperplane} of sets $X$ and $Y$ if $X \subset H_{d,\delta}^\le$ and $Y \subset H_{d,\delta}^\ge$.
        It is a \indx{strongly separating hyperplane} if $X \subset H_{d,\delta}^<$ and $Y \subset H_{d,\delta}^>$.
    \end{define}
    \begin{theorem}[Separation Theorem]
        Let $C \subset \reals^n$ be a closed, convex set, $x \in \reals^n$.
        Then
        \[
            x \not \in C \implies \text{there exists a hyperplane $H$ such that $H$ separates $\{x\}$ from $C$}.
        \]
    \end{theorem}
    \begin{proof}
        Take a closed radius around $x$, then the intersection $I$ of the ball and $C$ is closed and bounded so compact by Heine-Borel.
        Then the minimum of $||i-x||$ for $i \in I$ is attained by Weierstrass.
        Let $Z$ be the point in $I$ for which this distance is minimal.
        Take $d=(z-x)^\t $ and $\delta = \frac 1 2 d (z+x)$, then we find out if $dx>\delta$.
        Note that in general $||x||=xx^\t$.
        \[
            dx = d(\frac 1 2 (x+z) + \frac 1 2 (x-z)) = \frac 1 2 d (x+z) - \frac 1 2 d(z-x) = \delta - \frac 1 2 (z-x)^\t (z-x) = \delta - \delta - \frac 1 2 ||d||^2 < \delta.
        \]
        So now we have to prove that $dy>\delta$ for all $y \in C$.
        Suppose not for an $y$, then we need to show that $||y-x||<||z-x||$.
        Define $f(\lambda) = ||z-\lambda (y-z) -x ||^2$, which is a bit like the distance between a point on $[z,y]$ and $x$ because $z-\lambda(y-z) \in [z,y]$.
        Now we can write
        \begin{align*}
            f(\lambda)&= ((z-x)-\lambda(y-z))((z-x)-\lambda (y-z))^\t \\
            &=((z-x)-\lambda(y-z))\left( (z-x)^\t - \lambda (y-z)^\t  \right) \quad \text{because in general $(x-y)^\t = x^\t - y^\t$} \\
            &= (z-x)(z-x)^\t - \lambda (z-x)(y-z)^\t - \lambda (y-z)(z-x)^\t + \lambda^2 (y-z)(y-z)^\t \\
            &= ||z-x||^2 - 2\lambda (y-z) (z-x)^\t + \lambda ^2 ||y-z||^2 \quad \text{because in general $xy^\t = yx^\t $} \\
            &= ||d||^2 -2\lambda (y-z) d + \lambda ^2 ||y-z||^2.
        \end{align*}
        So
        \[
            f'(\lambda)= -2 (y-z) + 2 \lambda ||y-z||^2
        \]
        and
        \[
            f'(0) = -2 (y-z)d = -2 ||y-z||\cdot ||z-x|| \cos \alpha
        \]
        where $\alpha$ is the angle between $y-z$ and $d$.
        Now we see that since $y$ is on the other side of the hyperplane $dx=\delta$, the angle must be smaller than 90 degrees.
        Therefore $\cos \alpha >0$ and thus $f'(0)<0$.
        So for small $\lambda >0$ we have that $f(\lambda) < f(0)$.
        So
        \[
             ||z-x|| = f(0) > f(\lambda) = ||z-\lambda (y-z) -x ||
        \]
        and $u \coloneqq z-\lambda (y-z) \in [z,y]$ so $u \in C$.
        But $||u-x|| < ||z-x||$ so $z$ was not the point in $C$ closest to $x$.~$\lightning$
    \end{proof}
    \begin{theorem}[Farkas Lemma]
        Let $a_1,\dots ,a_n$ and $b$ be column vectors in $\reals^m$, then exactly one of the following is true.
        \begin{enumerate}
            \item $b \in \cone\{a_1,\dots,a_n\}$
            \item There is a row vector $d \in \reals^m$ such that $d a_i \ge 0$ for all $i$ and $db <0$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Suppose they are both true, so there exist $\lambda_i$ such that $\sumin \lambda_i a_i = b$ for $\lambda_i \ge 0$ and there is a $d$ such that $d a_i \ge 0$ and $db <0$.
        Then
        \[
            0 > db = d (\sumin \lambda_i a_i) = \sumin \lambda_i (d a_i) \ge 0\ \lightning
        \]
        Now we prove that at least one is true.
        Suppose 1 is not true, so $b \not \in \cone\{a_1,\dots,a_n\}$.
        By the separation theorem there exists a $d' \in \reals^m$ such that $d' a_i \ge 0$ for $a \in \cone\{a_1,\dots,a_n\}$ and $d'b <0$.
        Since $a_1,\dots,a_n \in \cone\{a_1,\dots,a_n\}$ we have $d' a_i \ge 0$ for all $i$ so 2 holds.
    \end{proof}
    \begin{question}
        Derive one variant of Farkas Lemma from an other.
    \end{question}
    \begin{answer}
        Good luck.
    \end{answer}
    \begin{theorem}[Farkas Lemma, variant]
        Let $A$ be an $m \times n$ matrix, $b \in \reals^m$, then exactly one of the following holds.
        \begin{enumerate}
            \item There exists an $x \in \reals^n$ such that $Ax\le b$
            \item There exists an $y \in \reals^m$ such that $yA=0,\ y\ge 0,\ yb<0$.
        \end{enumerate}
    \end{theorem}
    \begin{theorem}[Carath\'eodory's Theorem]
        Let $S \subset \reals^n$ be a finite set of vectors, if $x \in \cone S$ then there exists a linearly independent set $T \subset S$ such that $x \in \cone T$.
    \end{theorem}
    \begin{proof}
        Suppose $T$ was not linearly independent, then the $t \in T$ are dependent so you can write one $t$ as a linear combination of the others and subtract it.
        In other words, there exist $\mu_t \in \reals$ not all zero such that $\sum_{t \in T} \mu_t t =0$.
        Now we can combine the linear combinations, such that
        \begin{align*}
             \sum \lambda_t t &= x  \\ + \alpha \sum \mu_t t &= 0 \\ \hline \hline
            \sum (\lambda_t + \alpha \mu_t) t &= x
        \end{align*}
        Note that all $\lambda_i >0$ because if $\lambda_i = 0$ then we could remove the corresponding $t$ from $T$ and we would have a smaller $T$.

        So for $\alpha$ small enough, $\lambda_t + \alpha \mu_t >0$.
        Now let $\alpha$ grow until for one $t$ we have $\lambda_t + \alpha \mu_t =0$.
        But then there is a linear combination of $t$'s where one coefficient ($\lambda_t + \alpha \mu_t$) is zero, so we did not need that $t \ \lightning$.
    \end{proof}

    \section{Chapter 2}\label{sec:chapter2}
    \begin{theorem}[Strong Duality]
        \[
            \max \{ cx \mid Ax \le b \} = \min \{ yb \mid yA=c,\ y \ge 0 \}
        \]
        if the maximization problem is feasible and bounded.
    \end{theorem}
    \begin{proof}
        % todo
    \end{proof}
    \begin{question}
        Find the dual of $\min \{ yb \mid yA \ge c,\ y \ge 0 \}$.
    \end{question}
    \begin{answer}
        We try to write it in the form $\min \{ y'b \mid y'A'=c,\ y'\ge0 \}$, for example by introducting a slack variable $S$ so that the problem becomes
        \begin{align*}
            &\min \{ yb \mid yA -SI=c,\ y \ge 0,\ S \ge 0 \} \\
            =&\min \{ \matrix{y & S} \matrix{b \\ 0} \mid \matrix{y & S} \matrix{A \\ -I} = c,\ \matrix{y & S} \ge 0 \} \\
            =&\max \{ cx \mid \matrix{A \\ -I} x \le \matrix{b \\ 0} \} \\
            =&\max \{ cx \mid Ax\le b,\ x \ge 0 \}.
        \end{align*}
    \end{answer}
    \begin{theorem}[Complementary Slackness]
        Referring to the situation of strong duality,
        \[
            cx = yb \iff \text{ both solutions are optimal } \iff (Ax)_i = b_i \text{ or } y_i = 0\ \forall_i
        \]
        This means that if $x^*$ is an optimal solution to the primal problem, $y^*$ for the dual, if we plug $x^*$ into the $i$'th inequality of the primal problem and it turns out to have slack (``speling'' in Dutch), which means that equality does not hold, then the theorem says that $y_i^*$ has to be 0.

        This also holds in reverse, so if the $j$'th inequality of the dual has slack for $y^*$, then $x_j^* = 0$.
        Hence the name `complementary slack relation`.
    \end{theorem}
    \begin{question}
        Derive the optimal dual solution from the optimal primal solution $(5, 15)$ of the problem
        \[
            \min \{ 5x + 10y \mid x_1 + 3y\le 50,\ 4x+2y \le 60,\ x_1 \le 5,\ x_1,x_2 \ge 0 \}.
        \]
    \end{question}
    \begin{answer}
        If we plug $(5,15)$ into the primal problem we see that the first inequality is tight but for the second $4\cdot 5 + 2 \cdot 15 = 50 < 60$ so there is slack here, so $y_2^* =0$.
        
        The dual of this problem is 
        \[
            \max \{ 50 y_1 + 60 y_2 + 5 y_3 \mid y_1 + 4 y_2 + y_3 \ge 5,\ 3y_1 + 2y_2 \ge 10,\ y \ge 0 \}
        \]
        so since $x_2^* \neq 0$ we know that the corresponding inequality of the dual must be tight, so $3y_1^* + 2 \cdot 0 = 10$ so $y_1^* = 10/3$.
        Also $x_1^* \neq 0$ so $10/3 + y_3^* =5$ so $y^* = (10/3, 0, 5/3)$.
    \end{answer}
    \begin{theorem}
        Any linear optimization problem has a basic optimal solution.
    \end{theorem}

    \section{Chapter 3}\label{sec:chapter3}

    \begin{define}

        $\sumin \lambda_i x_i$ is an \indx{affine combination} of $x_i$ if $\sumin \lambda_i = 1$.

        $\sumin \lambda_i x_i$ is a \indx{convex combination} of $x_i$ if $\sumin \lambda_i = 1$ and $\lambda_i \ge 0$.

        The \indx{convex hull} of a set of vectors is the set of all convex combinations of subsets.
        In two dimensions, think of a flat surface spanned by points.

        A \indx{polyhedron} is $\{ x : Ax \le b \}$, the solution of a finite set of inequalities.

        A \indx{polytope} is the convex hull of a finite set of points.

        $F$ is a \indx{face} of a polyhedron $P$ if $F=P \cap H$ for some supporting hyperplane $H$, which touches $P$ but does not separate.

        A face $F$ is a \indx{facet} of $P$ if $\dim F = \dim P -1$.

        An \indx{edge} of $P$ is a bounded face $F$ such that $\dim F =1$.

        A \indx{ray} is an unbounded edge.

        A point $v \in P$ is a \indx{vertex} if $\{v\}$ is a face of $P$. $\V(P)$ is the set of vertices of $P$.

        The \indx{lineality space} of $P$ is the linear space $\lin(P) \coloneqq \{ d \in \reals^n : \{ x + \lambda d : \lambda \in \reals \} \subset P \ \forall_{x \in P} \}$, so the set of directions $d$ for which for all points $x \in P$ we have that $x + \lambda d$ is also in $P$.

        The \indx{cone of directions} of $P$ is the lineality space for $\lambda \ge 0$.

    \end{define}
    \begin{theorem}
        If $P$ is a bounded polyhedron, $P = \convhull \V(P)$.
    \end{theorem}
    \begin{proof}
        Assumpe $P$ is non-empty.
        \begin{itemize}
            \item ``$\supseteq$'' $\V(P) \subset P$ because $\forall_{v \in \V(P)} : \{ v\} = P \cap H$ for a hyperplane $H$.
            Because $P$ is convex ($P$ is the intersection of convex halfspaces), $\convhull \V(P) \subset P$.
            \item ``$\subseteq$'' Suppose note, let $x \in   $
        \end{itemize}
    \end{proof}

\end{document}